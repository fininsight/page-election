# -*- coding: utf-8 -*-
import scrapy
from navernews.items import NavernewsItem
from datetime import timedelta, date, datetime
import re
import requests


class NewsbycategorySpider(scrapy.Spider):
    name = 'newsbycategory'
    pre_url = 'https://news.naver.com/'
    start_urls=["https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=100"
    ,"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101"
    ,"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=102"
    ,"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=103"
    ,"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=104"
    ,"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=105"
    ]
    startdate = date.today()
    enddate = date.today()

    def start_requests(self):
        self.enddate = getattr(self, 'end', self.enddate)
        self.startdate = getattr(self, 'start', self.enddate)
        self.startdate = datetime.strptime(self.startdate, "%Y-%m-%d")
        self.enddate = datetime.strptime(self.enddate,"%Y-%m-%d")

        print("basedate" + "-" * 100)
        print(self.startdate)
        print(self.enddate)
        print("basedate" + "-" * 100)
        
        for u in self.start_urls:
            yield scrapy.Request(u, callback=self.parse_category)

    def parse_category(self, response) :
        def daterange(start_date, end_date):
            for n in range(int ((end_date - start_date).days+1)):
                yield start_date + timedelta(n)

        for d in daterange(start_date=self.startdate, end_date=self.enddate) :
            for url in response.css('ul.nav li a') :
                yield scrapy.Request(url=self.pre_url + url.attrib['href'] + "&page=1&date={}".format(d.strftime("%Y%m%d"))
                , callback=self.parse_list
                , meta={'category':url.xpath('text()').get(), 'date':d.strftime("%Y-%m-%d")})

    def parse_list(self, response):
        for url in response.css('div.list_body ul li dl dt:not(.photo) a::attr(href)').getall() :
            yield scrapy.Request(url=url, callback=self.parse, meta={'category':response.meta['category'], 'date':response.meta['date']})

        nextpage = int(re.search(r"(page=[0-9]*)", response.url).group().split('=')[1]) + 1        
        for page in response.css('div.paging a') :
            nextpageurl = int(re.search(r"(page=[0-9]*)", page.attrib['href']).group().split('=')[1])
            if nextpageurl == nextpage:                
                yield scrapy.Request(url=self.pre_url + "main/list.nhn" + page.attrib['href'], callback=self.parse_list, meta={'category':response.meta['category'], 'date':response.meta['date']})
                print("nextpage" + "*"*100)
                print(self.pre_url + page.attrib['href'])

    def parse(self, response):
        item = NavernewsItem()
        item['url'] = response.url
        item['category1'] = response.xpath("//div[@class='lnb_menu']/ul/li[@class='on']//span[@class='tx']//text()").get()   
        item['category2'] = response.meta['category']
        item['date'] = response.meta['date']
        item['title'] = response.xpath("//h3[@id='articleTitle']/text()").get().replace('"','')
        if item['title'] :
            item['title'] = item['title'].replace('"','')
        item['media'] = response.xpath("//div[@class='press_logo']/a/img/@alt").get()    
        item['inputdate'] = response.xpath("//div[@class='article_info']//span[@class='t11']/text()").get()
        item['summary'] = response.xpath("//strong[@class='media_end_summary']//text()").get()
        if item['summary'] :
            item['summary'] = item['summary'].replace('"','')
        cont = response.xpath("//div[@id='articleBodyContents']//text()").getall()   
        cont = ' '.join([item.strip() for item in cont ]) #좌우 공백 제거
        cont = cont[cont.find('{}')+3:] #javascript 함수 제거
        cont = re.sub(r"(\(.*\=)|(\[.*\=)|(\[.*\])","",cont) #기자 매체명 삭제
        #cont = cont[:cont.find(r'▶')] #특수문자
        cont = re.sub(r"(\w+([\.-]?\w+)*@\w+([\.-]?\w+)*(\.\w{2,3}))","",cont) #이메일 삭제
        cont = cont.replace('"','')
        item['content'] = '1' #cont.encode('utf-8')


        item['oricategory'] = '1' #response.xpath("//em[@class='guide_categorization_item']//text()").get()   

        #oid = re.search(r"(oid=[0-9]*)", response.url).group().split('=')[1]
        #aid = re.search(r"(aid=[0-9]*)", response.url).group().split('=')[1]
        #react_resp=requests.get('https://news.like.naver.com/v1/search/contents?suppress_response_codes=true&q=NEWS%5Bne_{0}_{1}%5D%7CNEWS_SUMMARY%5B{0}_{1}%5D%7CNEWS_MAIN%5Bne_{0}_{1}%5D&isDuplication=false'.format(oid,aid))
        #for react in react_resp.json()['contents'][0]['reactions']:
        #    item[react["reactionType"]] = int(react["count"])

        #comment_resp = requests.get('https://news.naver.com/api/comment/listCount.json?resultType=MAP&ticket=news&lang=ko&country=KR&objectId=news{0},{1}'.format(oid,aid))
        item['comment'] = '1' #comment_resp.json()['result']["news{0},{1}".format(oid,aid)]["comment"]

        item['updatedate'] = '1' #datetime.now().strftime("%Y-%m-%d %H:%M:%S")


        yield item
