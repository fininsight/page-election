{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face BPE Tokenizer\n",
    "https://github.com/huggingface/tokenizers/tree/master/bindings/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"./data/정치.csv\"]\n",
    "modelname = \"bpe_20200319\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습용 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356435"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "contents = []\n",
    "\n",
    "for file in files :\n",
    "    df = pd.read_csv(file)\n",
    "    contents = contents + df[\"content\"].tolist()\n",
    "\n",
    "len(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Piece Model (WPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) WPM 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/wpm/bpe_20200319-vocab.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train(files)\n",
    "tokenizer.save(\"./model/wpm/\", modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Load a BPE Model\n",
    "vocab = \"./model/wpm/{}-vocab.txt\".format(modelname)\n",
    "tokenizer = BertWordPieceTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) WPM 활용 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 44554\n",
      "1000 / 44554\n",
      "1000 / 44554\n",
      "1000 / 44554\n",
      "1000 / 44554\n",
      "1000 / 44554\n",
      "1000 / 44554\n",
      "1000 / 44554\n",
      "2000 / 44554\n",
      "2000 / 44554\n",
      "2000 / 44554\n",
      "2000 / 44554\n",
      "2000 / 44554\n",
      "2000 / 44554\n",
      "2000 / 44554\n",
      "2000 / 44554\n",
      "3000 / 44554\n",
      "3000 / 44554\n",
      "3000 / 44554\n",
      "3000 / 44554\n",
      "3000 / 44554\n",
      "3000 / 44554\n",
      "3000 / 44554\n",
      "3000 / 44554\n",
      "4000 / 44554\n",
      "4000 / 44554\n",
      "4000 / 44554\n",
      "4000 / 44554\n",
      "4000 / 44554\n",
      "4000 / 44554\n",
      "4000 / 44554\n",
      "4000 / 44554\n",
      "5000 / 44554\n",
      "5000 / 44554\n",
      "5000 / 44554\n",
      "5000 / 44554\n",
      "5000 / 44554\n",
      "5000 / 44554\n",
      "5000 / 44554\n",
      "5000 / 44554\n",
      "6000 / 44554\n",
      "6000 / 44554\n",
      "6000 / 44554\n",
      "6000 / 44554\n",
      "6000 / 44554\n",
      "6000 / 44554\n",
      "7000 / 44554\n",
      "6000 / 44554\n",
      "6000 / 44554\n",
      "7000 / 44554\n",
      "7000 / 44554\n",
      "7000 / 44554\n",
      "8000 / 44554\n",
      "7000 / 44554\n",
      "7000 / 44554\n",
      "7000 / 44554\n",
      "7000 / 44554\n",
      "8000 / 44554\n",
      "8000 / 44554\n",
      "9000 / 44554\n",
      "8000 / 44554\n",
      "8000 / 44554\n",
      "8000 / 44554\n",
      "9000 / 44554\n",
      "8000 / 44554\n",
      "8000 / 44554\n",
      "9000 / 44554\n",
      "10000 / 44554\n",
      "9000 / 44554\n",
      "10000 / 44554\n",
      "9000 / 44554\n",
      "10000 / 44554\n",
      "9000 / 44554\n",
      "9000 / 44554\n",
      "9000 / 44554\n",
      "11000 / 44554\n",
      "10000 / 44554\n",
      "11000 / 44554\n",
      "11000 / 44554\n",
      "10000 / 44554\n",
      "10000 / 44554\n",
      "10000 / 44554\n",
      "10000 / 44554\n",
      "12000 / 44554\n",
      "11000 / 44554\n"
     ]
    }
   ],
   "source": [
    "# Multiprocessing 으로 변경\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def tokenizeBySplit(contents) :\n",
    "    hangul = re.compile('[^ \\u3131-\\u3163\\uac00-\\ud7a3]+')\n",
    "\n",
    "    tokenized_text = []\n",
    "    cnt = 1\n",
    "\n",
    "    for content in contents :\n",
    "        try :\n",
    "            if content == np.nan :\n",
    "                continue \n",
    "                \n",
    "            text = hangul.sub('', content)\n",
    "            tokenized_text = tokenized_text + [w for w in tokenizer.encode(text).tokens if len(w) > 1]\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0 : \n",
    "                print(\"{} / {}\".format(cnt, len(contents)))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            #print(str(e))\n",
    "            #print(content)\n",
    "            \n",
    "    return tokenized_text\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "def tokenize(contents) :\n",
    "    chunk_size = int(len(contents)/multiprocessing.cpu_count())\n",
    "    li_split = [contents[i:i + chunk_size] for i in range(0, len(contents), chunk_size)]\n",
    "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "    ret = pool.map(tokenizeBySplit, li_split)\n",
    "    \n",
    "    tokenized_text = []\n",
    "    for text in ret :\n",
    "        print(text)\n",
    "        tokenized_text = tokenized_text + [token.replace(\"##\",\"\") for token in text if (token not in ('[CLS]','[SEP]','[UNK]')) & (len(token.replace(\"##\",\"\"))>1)]\n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "tokenized_text = tokenize(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 토큰 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def calFreqDist(tokenized_text) :\n",
    "    fdist=FreqDist(tokenized_text)\n",
    "    rc('font', family='AppleGothic')\n",
    "    fdist.plot(50)\n",
    "    return fdist\n",
    "\n",
    "fdist = calFreqDist(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import pandas as pd\n",
    "\n",
    "def genVoab(fdist, name) :\n",
    "    mecab = Mecab()\n",
    "    vocab = []\n",
    "\n",
    "    for w in fdist.most_common(10000) :\n",
    "        pos = mecab.pos(w[0])\n",
    "        if (pos[0][1][0] == \"N\") & (len(pos[0][0]) > 1) & (len(pos)>1) & (w[1]>100):\n",
    "            m = str(pos)\n",
    "            b = \"\".join([p[0] for p in pos])\n",
    "            p = \",\".join([p[1] for p in pos])\n",
    "            f = w[1]\n",
    "            #print(str(pos) + \" \" + str(w[1]) + \" \" + str(b))\n",
    "            vocab.append((m,p,b,f))\n",
    "\n",
    "    return pd.DataFrame(vocab, columns=['Mecab', 'PoS', name, 'Freq'])\n",
    "\n",
    "df = genVoab(fdist, \"WPM\")\n",
    "df.to_csv(\"./dict_wpm.csv\")\n",
    "df.head(50)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Piece Model (SPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/spm/bpe_20200319-vocab.json', './model/spm/bpe_20200319-merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train(files)\n",
    "tokenizer.save(\"./model/spm/\", modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "# Load a BPE Model\n",
    "vocab = \"./model/spm/{}-vocab.json\".format(modelname)\n",
    "merges = \"./model/spm/{}-merges.txt\".format(modelname)\n",
    "tokenizer = SentencePieceBPETokenizer(vocab, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenize(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = calFreqDist(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = genVoab(fdist, \"SPM\")\n",
    "df.to_csv(\"./dict_spm.csv\")\n",
    "df.head(50)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('자유한국당', 'NNP'),\n",
       " ('한국당', 'NNP'),\n",
       " ('예비', 'NNG'),\n",
       " ('후보', 'NNG'),\n",
       " ('때문', 'NNB'),\n",
       " ('에', 'JKB'),\n",
       " ('가능', 'NNG'),\n",
       " ('성', 'XSN'),\n",
       " ('국민', 'NNG'),\n",
       " ('들', 'XSN'),\n",
       " ('미래통합당', 'NNP'),\n",
       " ('필리버스터', 'NNP'),\n",
       " ('험지출마', 'NNP')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "mecab.pos('자유한국당한국당예비후보때문에가능성국민들미래통합당필리버스터험지출마') # 사전등록후 Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자유한국당',\n",
       " '한국당',\n",
       " '예비후보',\n",
       " '때문',\n",
       " '에',\n",
       " '가능성',\n",
       " '국민들',\n",
       " '미래통합당',\n",
       " '필리버스터',\n",
       " '험지',\n",
       " '출마</w>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('자유한국당한국당예비후보때문에가능성국민들미래통합당필리버스터험지출마').tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
