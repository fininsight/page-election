{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deoc2vec_cosinesim.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbp-MdeyxWom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "878ba4af-9ebb-4050-ca2b-fb0ab8f2aa76"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvg6nzsbxaMZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68044b5c-c8d2-4744-92f1-22287f303794"
      },
      "source": [
        "cd /content/drive/My Drive/fininsight/21대 총선분석/Text Classification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fininsight/21대 총선분석/Text Classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA_yYmaCxnvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install soynlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gw73gr5xZUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from soynlp.tokenizer import RegexTokenizer, LTokenizer, MaxScoreTokenizer\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRa9sEIlxouG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# khaii 설정 \n",
        "!git clone https://github.com/kakao/khaiii.git\n",
        "!pip install cmake\n",
        "!mkdir build\n",
        "!cd build && cmake /content/khaiii\n",
        "!cd /content/build/ && make all\n",
        "!cd /content/build/ && make resource\n",
        "!cd /content/build && make install\n",
        "!cd /content/build && make package_python\n",
        "!pip install /content/build/package_python\n",
        "\n",
        "# khaii 엮기 \n",
        "from khaiii import KhaiiiApi\n",
        "api = KhaiiiApi()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd90fjk8xpl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model\n",
        "doc_vectorizer = Doc2Vec.load('all_Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model')\n",
        "\n",
        "# load tagged_train_docs\n",
        "with open('tagged_train_docs.txt', 'rb') as f:\n",
        "  tagged_train_docs = pickle.load(f) # 단 한줄씩 읽어옴\n",
        "\n",
        "import numpy as np\n",
        "X = np.array([doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs])\n",
        "y = np.array([int(doc.tags) for doc in tagged_train_docs])\n",
        "\n",
        "with open('all_Doc2vec_matrix.pickle','wb') as f:\n",
        "    pickle.dump(X, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X= np.load('./Doc2Vec_Embedding.npy') # 불러오기 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsEKpJaJxu94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_cos(x):\n",
        "\n",
        "  Question = input('문장을 입력하세요.')\n",
        "\n",
        "  # 코사인 유사도 몇 이상인 것만 가져와라 \n",
        "  def condition_sorting(x, thresh):\n",
        "      idx = np.arange(x.size)[x > thresh]\n",
        "      return idx[np.argsort(x[idx])] \n",
        "\n",
        "  # 코사인 유사도 높은 순 index 추출 \n",
        "  def calculate_cos(matrix, target):\n",
        "      target_array = doc_vectorizer.infer_vector(target)\n",
        "      innerproduct = matrix.dot(target_array)\n",
        "      norm = np.linalg.norm(matrix, axis = 1) * np.linalg.norm(target_array)\n",
        "      cos = innerproduct / norm # 코사인 유사도 계산 \n",
        "      index = condition_sorting(cos, 0.4)[::-1][:200] # 유사도 0.4이상인 것 \n",
        "      return index\n",
        "\n",
        "  # 인풋 정제\n",
        "  def pre_fx(text):\n",
        "        # 이메일 등장하면, 그 뒷문장 모두 삭제 \n",
        "        text = re.sub(\"[a-zA-Z0-9]+\\@[a-zA-Z0-9]+\\.[a-z]{1,3}.[a-z]{1,3}.+\",'',text).strip()\n",
        "        text = re.sub('\\(.+?연합뉴스\\)','',text).strip() # (서울=연합뉴스)\n",
        "        text = re.sub('\\(.+?연합인포맥스\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?이데일리\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?조선일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?뉴시스\\)',' ',text).strip()\n",
        "        text = re.sub('\\(.+?뉴스1\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?SBS\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?오마이뉴스\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?중앙일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?매일경제\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?문화일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?세계일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?머니투데이\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?서울경제\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?데일리안\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?KBS\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?MBN\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?YTN\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?프레시안\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?디지털타임즈\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?국민일보(과거)\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?국민일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?헤드럴경제\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?한국일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?아이뉴스24\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?노컷뉴스\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?연합뉴스TV\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?서울일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?동아일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?한국경제\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?미디어오늘(과거)\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?미디어오늘\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?조세일보\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?파이낸셜뉴스\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?경향신문\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?채널A\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?머니S\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?TVCHOSUN\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?한겨례\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?전자신문\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?SBSCNBC\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?한국경제TV\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?조선비즈\\)','',text).strip()\n",
        "        text = re.sub('\\(.+?ZDNetKorea\\)','',text).strip()\n",
        "        text = re.sub('\\[.+?\\]','',text).strip()\n",
        "        text = re.sub('\\(사진.+?\\)','',text).strip()\n",
        "        # 신문사 이름 삭제 \n",
        "        text = re.sub('조선일보|뉴시스|아시아경제|이데일리|뉴스1|SBS|오마이뉴스|중앙일보|매일경제|문화일보|세계일보|머니투데이|서울경제|데일리안|KBS|MBN|YTN|프레시안|디지털타임스|국민일보(과거)|국민일보|헤럴드경제|한국일보|아이뉴스24|노컷뉴스|연합뉴스TV|서울일보|동아일보|한국경제|미디어오늘(과거)|미디어오늘|조세일보|파이낸셜뉴스|경향신문|채널A|머니S|TVCHOSUN|한겨례|전자신문|SBSCNBC|한국경제TV|조선비즈|ZDNetKorea','',text).strip()\n",
        "        # 000 기자, 000  가자, 000기자 삭제\n",
        "        text = re.sub('[ㄱ-힑]+ 기자|[ㄱ-힑]+  기자|[ㄱ-힑]+기자','',text).strip()    \n",
        "        text = re.sub('[a-zA-Z0-9]{1,20}\\@[a-zA-Z0-9]{1,20}\\.[a-z]{1,20}.+','',text).strip()    \n",
        "\n",
        "        return text\n",
        "\n",
        "    def filter(s):\n",
        "        hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글과 띄어쓰기를 제외한 모든 글자\n",
        "        result = hangul.sub('', s) # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
        "        return result\n",
        "\n",
        "  Question  = pre_fx(Question)\n",
        "  Question = filter(Question)\n",
        " \n",
        "  # khaii 엮기 \n",
        "  from khaiii import KhaiiiApi\n",
        "  api = KhaiiiApi()\n",
        "\n",
        "  new_question = []\n",
        "  for word in api.analyze(Question): \n",
        "      for morph in word.morphs:\n",
        "          new_question.append(morph.lex)\n",
        "\n",
        "  df = pd.read_pickle(\"final_Khaiii.pickle\")\n",
        "\n",
        "  # '상위 1 class :\\n', \n",
        "  return  'category 1:', df.iloc[calculate_cos(X, new_question)[0]]['category1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg-Gy43GyBpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_cos('x')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}