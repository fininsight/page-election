{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kobertner_khaiii_keyword.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "f8-xQ9NNJuAu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqCx94N9Gkmg",
        "colab_type": "text"
      },
      "source": [
        "# 환경설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC6-P5ev4JvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/kakao/khaiii.git\n",
        "!pip install cmake\n",
        "!mkdir build\n",
        "!cd build && cmake /content/khaiii\n",
        "!cd /content/build/ && make all\n",
        "!cd /content/build/ && make resource\n",
        "!cd /content/build && make install\n",
        "!cd /content/build && make package_python\n",
        "!pip install /content/build/package_python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WovX9q7K5NRE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "77803a7d-1c37-44b6-8c09-bb2ec9e39c74"
      },
      "source": [
        "from khaiii import KhaiiiApi\n",
        "api = KhaiiiApi()\n",
        "for word in api.analyze(\"정세균을 분석해보자.\"):\n",
        "    print(word)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정세균을\t정/NNG + 세균/NNP + 을/JKO\n",
            "분석해보자.\t분석/NNG + 하/XSV + 여/EC + 보/VX + 자/EF + ./SF\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayZxNcBF8ujm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install WordCloud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5yxQ9xr8v7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.font_manager as fm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYewNVKL8whI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys_font=fm.findSystemFonts()\n",
        "print(f\"sys_font number: {len(sys_font)}\")\n",
        "print(sys_font)\n",
        "\n",
        "nanum_font = [f for f in sys_font if 'Nanum' in f]\n",
        "print(f\"nanum_font number: {len(nanum_font)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrAwWPbu8wbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHBuZZnD8zJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 체크해보면 폰트 개수가 늘어났다\n",
        "sys_font=fm.findSystemFonts()\n",
        "print(f\"sys_font number: {len(sys_font)}\")\n",
        "\n",
        "nanum_font = [f for f in sys_font if 'Nanum' in f]\n",
        "print(f\"nanum_font number: {len(nanum_font)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOHw0SbG5Oiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQveirk85OgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAf-4vhr5Od0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cd /content/drive/My Drive/fininsight/21대 총선분석/TexkRank/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tR80CB7qod",
        "colab_type": "text"
      },
      "source": [
        "# KoBERT NER 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es409u7E5Qvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_hwang = pd.read_csv('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/황교안_ner.csv')\n",
        "df_lee = pd.read_csv('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/이낙연_ner.csv')\n",
        "df_jongro = pd.read_csv('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/종로구_ner.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h21LZU8Y4LSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ner_clean(x):\n",
        "  for decode in x:\n",
        "    de = ' '.join([x for x in decode.split(' ') if x[0] =!'<' and x[-1] != '>'])\n",
        "  return de"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPxLBouF7FDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_hwang['ner_clean'] = df_hwang['content_ner'].apply(ner_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvEyctET7FBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lee['ner_clean'] = df_lee['content_ner'].apply(ner_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1mRVPjN7E-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_jongro['ner_clean'] = df_jongro['content_ner'].apply(ner_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdLIPao47p2J",
        "colab_type": "text"
      },
      "source": [
        "# Khaiii 사용자 사전 추가 & 복합명사 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezHewQvVI71t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모듈화\n",
        "\n",
        "import FINModule.KhaiiiToken as kt\n",
        "DF = kt.khaiii_complexnoun(df_lee)\n",
        "df = DF.khaiii_complex('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/황교안_kobertner_khaiii.csv')\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzmrdIDcJlLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모듈화\n",
        "\n",
        "import FINModule.KhaiiiToken as kt\n",
        "DF = kt.khaiii_complexnoun(df_hwang)\n",
        "df = DF.khaiii_complex('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/이낙연_kobertner_khaiii.csv')\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWPDBVrtJlHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모듈화\n",
        "\n",
        "import FINModule.KhaiiiToken as kt\n",
        "DF = kt.khaiii_complexnoun(df_jongro)\n",
        "df = DF.khaiii_complex('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/종로구_kobertner_khaiii.csv')\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8-xQ9NNJuAu",
        "colab_type": "text"
      },
      "source": [
        "#### 명사추출 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69YGoGCA7S2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from khaiii import KhaiiiApi\n",
        "# api = KhaiiiApi()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AbHJhte7S0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def khaiii_complex(sentence):\n",
        "#   lex = []\n",
        "#   tag = []\n",
        "\n",
        "#   for word in api.analyze(str(sentence)):\n",
        "#     for m in word.morphs:\n",
        "#       lex.append(m.lex)\n",
        "#       tag.append(m.tag)\n",
        "\n",
        "#   complex_nouns = []\n",
        "#   for i in range(len(tag)-1):\n",
        "#     if tag[i][0] == 'N' and tag[i+1][0] == 'N':\n",
        "#       complex_nouns.append(lex[i] + ' ' + lex[i+1])\n",
        "#     elif tag[i] in ['NNG', 'NNP', 'NNB', 'NR', 'NP', 'VA'] and len(lex[i])>1:\n",
        "#       complex_nouns.append(lex[i])\n",
        "\n",
        "#   return complex_nouns    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_qDV6Y7Sxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_hwang['ner_khaiii_token'] = df_hwang['ner_clean'].apply(khaiii_complex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lMST5IC7E8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_hwang.to_csv('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/황교안_kobertner_khaiii.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iuGRodv8Dak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lee['ner_khaiii_token'] = df_lee['ner_clean'].apply(khaiii_complex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBpo90yL8DYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lee.to_csv('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/이낙연_kobertner_khaiii.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbPLtI7Y8DWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_jongro['ner_khaiii_token'] = df_jongro['ner_clean'].apply(khaiii_complex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFp033yM8DUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_jongro.to_csv('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/종로구_kobertner_khaiii.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TePoE-L8Rk0",
        "colab_type": "text"
      },
      "source": [
        "# TexkRANK Keyword 추출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_OiZSsj8Zyd",
        "colab_type": "text"
      },
      "source": [
        "## 1. 황교안"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVthF7nn8eBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from FINModule.TextRank import TextRank \n",
        "tr = TextRank(df_hwang)\n",
        "hwang_count = tr.count_list\n",
        "hwang_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thj0SKF08eAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/황교안_kokhaiii_keyword', 'wb') as handle:\n",
        "  pickle.dump(hwang_count, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJDstuWd8d-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',background_color='white', width=1200, height=1000).generate_from_frequencies(count_list) \n",
        "wordcloud = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf', background_color=\"white\", width=1200, height=1000).generate_from_frequencies(hwang_count) \n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\") \n",
        "plt.figure(figsize = (20,20))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9URqKq49L5Z",
        "colab_type": "text"
      },
      "source": [
        "## 이낙연"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3S7iLILANa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from FINModule.TextRank import TextRank \n",
        "tr = TextRank(df_lee)\n",
        "lee_count = tr.count_list\n",
        "lee_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsfZfP1t9VRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/이낙연_kokhaiii_keyword', 'wb') as handle:\n",
        "  pickle.dump(lee_count, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AQT6bua9VMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',background_color='white', width=1200, height=1000).generate_from_frequencies(count_list) \n",
        "wordcloud = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf', background_color=\"white\", width=1200, height=1000).generate_from_frequencies(lee_count) \n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\") \n",
        "plt.figure(figsize = (20,20))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei93UOZ29lRw",
        "colab_type": "text"
      },
      "source": [
        "## 종로구"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fkn7mJW9VLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from FINModule.TextRank import TextRank \n",
        "tr = TextRank(df_hwang)\n",
        "hwang_count = tr.count_list\n",
        "hwang_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZWZmyb09VGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/fininsight/21대 총선분석/TexkRank/Data/종로구_kokhaiii_keyword', 'wb') as handle:\n",
        "  pickle.dump(jongro_count, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyxXgLgZ9kPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',background_color='white', width=1200, height=1000).generate_from_frequencies(count_list) \n",
        "wordcloud = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf', background_color=\"white\", width=1200, height=1000).generate_from_frequencies(jongro_count) \n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\") \n",
        "plt.figure(figsize = (20,20))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECJJprj7J9l5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPbGLAmM9VI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def scan_vocabulary(sents, min_count=2):\n",
        "    sen = [sent for sent in sents[:5]]\n",
        "    counter = Counter([y for x in sen for y in x]) \n",
        "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
        "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
        "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
        "    return idx_to_vocab, vocab_to_idx\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2):\n",
        "    counter = defaultdict(int)\n",
        "    for s, tokens_i in enumerate(tokens):\n",
        "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
        "        n = len(vocabs)\n",
        "        for i, v in enumerate(vocabs):\n",
        "            if window <= 0:\n",
        "                b, e = 0, n\n",
        "            else:\n",
        "                b = max(0, i - window)\n",
        "                e = min(i + window, n)\n",
        "            for j in range(b, e):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                counter[(v, vocabs[j])] += 1\n",
        "                counter[(vocabs[j], v)] += 1\n",
        "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
        "    n_vocabs = len(vocab_to_idx)\n",
        "    return dict_to_mat(counter, n_vocabs, n_vocabs)\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def dict_to_mat(d, n_rows, n_cols):\n",
        "    rows, cols, data = [], [], []\n",
        "    for (i, j), v in d.items():\n",
        "        rows.append(i)\n",
        "        cols.append(j)\n",
        "        data.append(v)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "def word_graph(sents, min_count=2, window=2, min_cooccurrence=2):\n",
        "    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, min_count)\n",
        "#     tokens = [y for x in sen for y in x]\n",
        "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence) # , cooccurrence\n",
        "    return g, idx_to_vocab\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def pagerank(x, df=0.85, max_iter=30):\n",
        "    assert 0 < df < 1\n",
        "\n",
        "    # initialize\n",
        "    A = normalize(x, axis=0, norm='l1')\n",
        "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
        "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
        "\n",
        "    # iteration\n",
        "    for _ in range(max_iter):\n",
        "        R = df * (A * R) + bias\n",
        "\n",
        "    return R\n",
        "\n",
        "def textrank_keyword(sents,  min_count, window, min_cooccurrence, df=0.85, max_iter=30, topk=30):\n",
        "    g, idx_to_vocab = word_graph(sents,  min_count, window, min_cooccurrence)\n",
        "    R = pagerank(g, df, max_iter).reshape(-1)\n",
        "    idxs = R.argsort()[-topk:]\n",
        "    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]\n",
        "    return keywords\n",
        "\n",
        "sents = df['khaiii_complex']\n",
        "\n",
        "count_lists = textrank_keyword(sents, min_count=2, window=2, min_cooccurrence=2, df=0.85, max_iter=30, topk=30)\n",
        "\n",
        "jongro_count = {}\n",
        "for x in count_lists:\n",
        "    jongro_count[x[0]] = x[1]\n",
        "    \n",
        "jongro_count"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}