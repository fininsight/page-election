# -*- coding: utf-8 -*-
"""textrank_summa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZuHP6l7f8zkUwklQeysvO5m01Nhn7B9f
"""

import pandas as pd

train = pd.read_csv('ready_for_train_dataset_100000_20170601_20180601.csv')
# train = pd.read_csv('ready_for_train_dataset_100000_20170601_20180601.csv')
train.head()

import pandas as pd

test = pd.read_csv('ready_for_test_dataset_50000_20190601_20191031.csv')
# test = pd.read_csv('ready_for_test_dataset_1000_20200101_20200315.csv')
test.head()

import nltk
nltk.download('punkt')

import pickle
import nltk
from nltk.tokenize import sent_tokenize
from konlpy.tag import Mecab
import networkx as nx
import math
import matplotlib.pyplot as plt
import numpy as np
import re

korean_patten = re.compile('[^가-힣]')


class TextRank :
    def __init__(self, tokenizer = None, exceptional_stop_pos =[]):
        self.stop_pos = ['IC', 'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JC','JX'
                    , 'XR', 'SF', 'SE', 'SSO', 'SSC', 'SC', 'SY', 'EC', 'EF', 'ETN', 'ETM', 'XSV', 'XSA', 'XSN', 'XPN']
        
        if not tokenizer:
             self.tokenizer =  Mecab()
        else:
            self.tokenizer = tokenizer

        if not exceptional_stop_pos:
            self.stop_pos = [x for x in self.stop_pos if x not in exceptional_stop_pos]
        

    def pos_tagging(self, content, category="정치"):
        def subtokenize(pos_list, dct) :
          pos_str = "[" + ", ".join([str(t) for t in pos_list]) + "]"
          for pattern in dct :
            src = ", ".join([str(i) for i in pattern[1]])
            tgt = str(pattern[0])
            pos_str = pos_str.replace(src, tgt)

          tokenized_text = eval(pos_str)
          return tokenized_text

        def group_by_pos(tokens, join_char='') :
            return (join_char.join([t[0] for t in tokens]), 'NNP' if len(set([t[1] for t in tokens])) > 2 else tokens[-1][1] )

        def gen_tokens(tokens, join_char='', group_by_pos_li = ['NNP', 'NNG', 'SN', 'SH', 'SL', 'NNBC'], stop_pos=[]) :
            #last_token = tokens[0]

            ret = []
            li = []
            for t in tokens :
                if t[1] in group_by_pos_li : li.append(t)
                else :
                    if len(li) > 0 : ret.append(group_by_pos(li, join_char))
                    if t[1] not in stop_pos : ret.append(t)
                    li=[]
            if len(li) > 0 : ret.append(group_by_pos(li, join_char))

            return ret

        morphs_dict = pickle.load(open('morphs_dict.pickle',"rb"))
        comp_dict = pickle.load(open('comps_dict.pickle',"rb"))
        ret = []
        for s in sent_tokenize(content) :
            #ret.append((s, sent_li))
            sent_li= []
            sent_li.append([(w, subtokenize(subtokenize(gen_tokens(self.tokenizer.pos(w), stop_pos=self.stop_pos), morphs_dict[category]), comp_dict[category])) for w in s.split()])

            #for w in s.split() : sent_li.append((w, gen_tokens(tokenizer.pos(w))))
            ret.append((s, sent_li))
            #ret.append((s, [t for t in gen_tokens(sent_li, join_char= ' ') if t[1] not in stop_pos]))

        return ret
        
    def keywords(self, text, n=10) :     
        tokens = self.pos_tagging(text)
        
        tokens = [t for s in tokens for w in s[1] for t in w]
        nodes = [k for t in tokens for k in t[1] if (k[1][0] in ['N', 'V']) & (len(k[0])>1)]
        tokens = [k for t in tokens for k in t[1]]
        
        def connect(nodes, tokens) :            
            window_size = 5 # coocurrence를 판단하기 위한 window 사이즈 설정

            edges = []
            for window_start in range(0,(len(tokens)-window_size+1)):
                window = tokens[window_start:window_start+window_size]            
                #edges.append([(window[i], window[j]) for i in range(window_size) for j in range(window_size) if ( (i > j) & (window[i] in nodes) & (window[j] in nodes))])

                for i in range(window_size) :
                    for j in range(window_size) : 
                        if (i > j) & (window[i] in nodes) & (window[j] in nodes) :            
                            edges.append((window[i], window[j]))
            return edges

        graph=nx.diamond_graph()
        graph.clear() #처음 생성시 graph에 garbage node가 남아있어 삭제
        graph.add_nodes_from(list(set(nodes))) #node 등록
        graph.add_edges_from(connect(nodes, tokens)) #edge 연결
        scores = nx.pagerank(graph) #pagerank 계산
        rank = sorted(scores.items(), key=lambda x: x[1], reverse=True) #score 역순 정렬
        return rank[:n]
    
    def print_keywords(self, text, n=10) :   
        print("Keyword : ")
        for k in self.keywords(text, n) :
            print("{} - {}".format(k[0][0], k[1]))
    
    def summarize(self, text, n=3) :
        tokens = self.pos_tagging(text)
        #자카드 유사도 계산
        def jaccard_similarity(query, document):
            intersection = set(query).intersection(set(document))
            union = set(query).union(set(document))
            return len(intersection)/len(union)

        # 문장간 유사도 측정 (BoW를 활용 코사인 유사도 측정)
        def sentence_similarity(sentence1, sentence2):
    
            sentence1 = self.tokenizer.morphs(sentence1[0])#[t[0] for s in sentence1[1][0] for t in s[1] if t[1][0] in ['N','V'] ] 
            sentence2 = self.tokenizer.morphs(sentence2[0])#.split()#[t[0] for s in sentence2[1][0] for t in s[1] if t[1][0] in ['N','V'] ]
            #print(sentence1)
            return jaccard_similarity(sentence1, sentence2)

        def sentences(doc):
            return [s[0].strip() for s in doc]

        def connect(doc):
            return [(start[0].strip(),end[0].strip() ,sentence_similarity(start, end)) 
                    for start in doc for end in doc if start is not end]
        
        graph=nx.diamond_graph()
        graph.clear() #처음 생성시 graph에 garbage node가 남아있어 삭제
        graph.add_nodes_from(sentences(tokens)) #node 등록
        graph.add_weighted_edges_from(connect(tokens)) #edge 연결
        scores = nx.pagerank(graph) #pagerank 계산
        #print(scores)
        rank = sorted(scores.items(), key=lambda x: x[1], reverse=True) #score 역순 정렬
        ssum = rank[:n]
        ranks = []
        for s in ssum:
          ranks.append(s[0])
        return ' '.join(ranks)

print('summarizing')
summarized = []

for con in train['Content']:
  t = TextRank()
  summarized.append(t.summarize(con, 3))

# print(train.shape)
print(len(summarized))

train['summarized'] = summarized
train.head(3)

train.to_csv('summarized_dataset_train_m.csv', index=False)

train = train.loc[:,['Unnamed: 0', 'Category','summarized']]
train = train.rename(columns = {'Unnamed: 0':'id', 'Category':'label', 'summarized':'document'})
train.head(3)

train.to_csv('dataset_train_summarized_m.txt',

                 sep='\t',

                 na_rep='NaN', 

                 columns = ['id', 'document', 'label'], # columns to write

                 index = False) # do not write index

print('save traintset')

summarized_test = []

for con in test['Content']:
  t = TextRank()
  summarized_test.append(t.summarize(con, 3))

print(test.shape)
print(len(summarized_test))

test['summarized'] = summarized_test
test.head(3)

test.to_csv('summarized_dataset_test_m.csv', index=False)

test = test.loc[:,['Unnamed: 0', 'Category','summarized']]
test = test.rename(columns = {'Unnamed: 0':'id', 'Category':'label', 'summarized':'document'})
test.head(3)

test.to_csv('dataset_test_summarized_m.txt',

                 sep='\t',

                 na_rep='NaN', 

                 columns = ['id', 'document', 'label'], # columns to write

                 index = False) # do not write index

print('save testset')

